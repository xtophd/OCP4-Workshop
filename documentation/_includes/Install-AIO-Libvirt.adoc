:gitrepo: https://github.com/xtophd/OCP-Workshop
:includedir: _includes
:doctype: book
:sectnums:
:sectnumlevels: 3
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= All In One: AIO Libvirt

[discrete]
== Overview

If you plan to deploy OCP in a libvirt/kvm based environment, you are in luck.  These playbooks can automate everything including the setup of the libvirt/kvm host itself.

Using this configuration, Openshift will be deployed as virtual machines in a NAT'd internal network.  You can easily change this to a bridged network by using alternate configuration templates.

NAT is easy because:

  * dhcp, pxe don't conflict with public network
  * it's a real cloud, meaning you get ingress through the haproxy loadbalancer.  The outside world cannot reach your cluster nodes directly.

After setup of the libvirt host is complete, the bastion host is built from a DVD and post-configured with ansible plays to enable:

  * dns
  * dhcp
  * pxe
  * matchbox
  * nfs for persistent storage (not supported in PROD, but works for POC)
  * openshift install/ignition generation
  * more...

Lastly, these are default configurations on the virt host but can be changed in the config files:

  * vm images will go into /home/virt-images
  * iso images will got into /home/iso
  * network is NAT by default with 192.168.123.xxx and ocp.example.com as internal domain.

Virt host should be loaded with either RHEL 7.7+ or RHEL 8.1+



== Prerequisites

  * RHEL 7.7+ or 8.0+ base installation
  * Installed packages: git, ansible 2.9
  * RHEL 7.9 or 8.4 DVD ISO saved in /home/iso
  * RHEL 7.9 or 8.4 BOOT ISO saved in /home/iso
  * log on to cloud.redhat.com and get your pull-secret.txt
  * internet access (either direct or via proxy)

=== Register system

However you get it done, the primary virthost system will need access to additional RHEL repos.  At this time, I'm testing on RHEL 7.9 and RHEL 8.4.

For RHEL 7.9

    - "rhel-7-server-rpms"
    - "rhel-7-server-optional-rpms"
    - "rhel-7-server-ansible-2.9-rpms"

For RHEL 8.4:

    - "rhel-8-for-x86_64-baseos-rpms"
    - "rhel-8-for-x86_64-appstream-rpms"
    - "ansible-2.9-for-rhel-8-x86_64-rpms"
 
 And yes, I test with CentOS 7 from time to time and it should work as both the virthost platform and for the bastion server.
 
=== Install additional packages

WARNING: this process requires and is tested with ansible 2.9.  Anything older that Ansible 2.9 *WILL NOT WORK!!!*

Besides an @Base installation, you will need git and ansible installed before you begin.  Once installed, the playbooks will take care of the rest.

----
yum -y install git,ansible
----

=== Git Clone Repo

Still on your virt host:

----
cd ~

git clone http://github.com/xtophd/OCP4-Workshop --recurse-submodules
----

=== Create the Config Files

==== Determine Network Configuration

In both cases (NAT or Bridge), the libvirt 'default' network is left alone and unused.  This allows for the continued use of whatever you may already have configured on the virt host without conflict with what is about to be deployed.

In this example, we will use the NAT'd configuration.

==== Copy NAT configurations to './config' directory

Using the NAT'd network config will require the least amount of changes to the config.  You really only need to provide the provide proper info for IP address space.

----
cd ~/OCP4-Workshop/config

cp ../sample-configs/libvirt-nat/* .
----

=== Edit the Config Files

NOTE: Any changes you make to the files in './config/' will be left untouched in the event you update the project repo with `git pull`.  Git is configured to ignore files in ../config

WARNING: Adjusting the default dns and timeserver for your home/lab is critical and is almost certainly mandatory.  You will find those parameters in master-config.yml (network_nameserver and network_timeserver)

MAC addresses for a libvirt deployment are automatically generated.  You only need to create/edit MAC addresses if you want to assign them yourself.

Just like the name implies 'master-config.yml' is the main configuration file.  Most parameters should be straight forward, but
in particular you need to:

  * fix hostnames, mac addresses & ip addresses
  * set or comment out h_rhcosDEV and r_rhcosNIC accordingly

The majority of configuration options are documented in the config files themselves.

It is paramount that you update the configs for your environment.  Please pay close attention to the general network information, and of course the IP and MAC addresses for your systems (vms).

  + master-config.yml
  + xtoph-deploy-config.yml
  + credentials.yml

=== Retrieve RHEL 8.4 DVD

The utility server currently depends on a RHEL 8.4 DVD image.  You can copy the URL from the access.redhat.com download page, and be sure to use single quotes to prevent bash from parsing the string.

----
mkdir -p /home/iso/

wget -O /home/iso/rhel-8.4-x86_64-dvd.iso '<URL>'
wget -O /home/iso/rhel-8.4-x86_64-boot.iso '<URL>'

----

=== Retrieve Openshift Pull Secret

WARNING: The file /root/OCP4-Workshop/config/pull-secret.txt must exist on the deployer host before you continue.

NOTE: Pull secret URL: https://cloud.redhat.com/openshift/install/metal/user-provisioned

Using a browser, go to redhat.com and retrieve the pull-secret.  You can either:

    - download the pull-secret.txt and transfer it to the config directory, OR
    - copy the pull-secret.txt to the paste buffer and then edit pull-secret.txt and paste the contents

=== Final Check List

    - Is there enough memory?
    - Is there enough cpu?
    - Is there enough disk space and is it in the right location?
    - Is the RHEL ISO downloaded and stored in the right place
    - Is the Openshift UPI Pull Secret downloaded and stored in the right place
    - Did you install (upgrade) Ansible 2.9


== Platform Setup

----
./xtoph-deploy.sh setup
----

== OCP Deployment

----
./xtoph-deploy.sh deploy
----

== Post Installation

If you elected to deploy the workshop materials in your configuartion, you will find a number of convenient scripts in /usr/local/bin.

=== Extra Info

Artifacts from the build will be on the bastion host in /root/ocp-<clustername>

Authority files will be on the bastion host in /root/ocp-<clustername>/auth

[discrete]
== Continue to the Workshop

The workshop document and exercises can be found here:

* link:{docsdir}/OCP-Workshop.adoc[OCP-Workshop Hands-on Lab]

[discrete]
= The End

.Built-in
asciidoctor-version:: {asciidoctor-version}
safe-mode-name:: {safe-mode-name}

////
Always end files with a blank line to avoid include problems.
////
